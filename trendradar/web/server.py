"""
TrendRadar Web Viewer Server

æä¾›åŸºäº Web çš„æ–°é—»åˆ†ç±»æŸ¥çœ‹å™¨ç•Œé¢
æ”¯æŒå®šæ—¶è‡ªåŠ¨è·å–æœ€æ–°æ•°æ®
"""

import asyncio
import os
import sqlite3
import sys
import time
from collections import deque
from datetime import datetime, date, timedelta
from pathlib import Path
from threading import Lock
from typing import Optional
from urllib.parse import unquote

from fastapi import FastAPI, Request, Query
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import HTMLResponse, JSONResponse, Response
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import json
import requests

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
# trendradar/web/server.py -> trendradar/web -> trendradar -> hotnews (é¡¹ç›®æ ¹ç›®å½•)
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from trendradar.web.news_viewer import NewsViewerService
from mcp_server.services.data_service import DataService
from mcp_server.services.cache_service import get_cache
from trendradar.crawler import DataFetcher
from trendradar.core import load_config
from trendradar.storage import convert_crawl_results_to_news_data

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="TrendRadar News Viewer", version="1.0.0")

# å¯ç”¨ Gzip å‹ç¼©ï¼ˆå“åº”å¤§äº 500 å­—èŠ‚æ—¶å‹ç¼©ï¼‰
app.add_middleware(GZipMiddleware, minimum_size=500)

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•ï¼ˆå¸¦ç¼“å­˜æ§åˆ¶ï¼‰
static_dir = Path(__file__).parent / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")


# é™æ€èµ„æºç¼“å­˜ä¸­é—´ä»¶
@app.middleware("http")
async def add_cache_headers(request: Request, call_next):
    response = await call_next(request)
    path = request.url.path
    
    # ä¸ºé™æ€èµ„æºæ·»åŠ ç¼“å­˜å¤´
    if path.startswith("/static/"):
        # CSS/JS æ–‡ä»¶ç¼“å­˜ 1 å°æ—¶ï¼ˆå¼€å‘æœŸé—´ï¼‰ï¼Œç”Ÿäº§ç¯å¢ƒå¯è®¾æ›´é•¿
        response.headers["Cache-Control"] = "public, max-age=3600"
    
    return response

_FETCH_METRICS_MAX = 5000
_fetch_metrics = deque(maxlen=_FETCH_METRICS_MAX)
_fetch_metrics_lock = Lock()

_last_platform_content_keys = {}


def _metrics_file_path() -> Path:
    return project_root / "output" / "metrics" / "fetch_metrics.jsonl"


def _append_fetch_metrics_batch(metrics):
    if not metrics:
        return

    try:
        fp = _metrics_file_path()
        fp.parent.mkdir(parents=True, exist_ok=True)
        with fp.open("a", encoding="utf-8") as f:
            for m in metrics:
                f.write(json.dumps(m, ensure_ascii=False) + "\n")

        try:
            lines = fp.read_text(encoding="utf-8").splitlines()
        except Exception:
            lines = []
        if len(lines) > _FETCH_METRICS_MAX:
            fp.write_text("\n".join(lines[-_FETCH_METRICS_MAX:]) + "\n", encoding="utf-8")
    except Exception:
        return


def _record_fetch_metrics(metrics):
    if not metrics:
        return

    with _fetch_metrics_lock:
        for m in metrics:
            _fetch_metrics.append(m)


# è‡ªå®šä¹‰ JSONResponse ç±»ï¼Œç¡®ä¿ä¸­æ–‡æ­£ç¡®æ˜¾ç¤º
class UnicodeJSONResponse(Response):
    media_type = "application/json"
    
    def render(self, content) -> bytes:
        return json.dumps(
            content,
            ensure_ascii=False,
            allow_nan=False,
            indent=None,
            separators=(",", ":"),
        ).encode("utf-8")

# é…ç½®æ¨¡æ¿ç›®å½•
templates_dir = Path(__file__).parent / "templates"
templates_dir.mkdir(exist_ok=True)
templates = Jinja2Templates(directory=str(templates_dir))

# å…¨å±€æœåŠ¡å®ä¾‹
_viewer_service: Optional[NewsViewerService] = None
_data_service: Optional[DataService] = None

# å®šæ—¶ä»»åŠ¡çŠ¶æ€
_scheduler_task: Optional[asyncio.Task] = None
_scheduler_running: bool = False
_last_fetch_time: Optional[datetime] = None
_fetch_interval_minutes: int = 30  # é»˜è®¤30åˆ†é’Ÿè·å–ä¸€æ¬¡

_online_db_conn: Optional[sqlite3.Connection] = None


def _get_online_db_conn() -> sqlite3.Connection:
    global _online_db_conn

    if _online_db_conn is not None:
        return _online_db_conn

    output_dir = project_root / "output"
    output_dir.mkdir(parents=True, exist_ok=True)
    db_path = output_dir / "online.db"

    conn = sqlite3.connect(str(db_path), check_same_thread=False)
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.execute(
        "CREATE TABLE IF NOT EXISTS online_sessions (session_id TEXT PRIMARY KEY, last_seen INTEGER NOT NULL)"
    )
    conn.commit()

    _online_db_conn = conn
    return conn


def get_services():
    """è·å–æˆ–åˆå§‹åŒ–æœåŠ¡å®ä¾‹"""
    global _viewer_service, _data_service
    
    if _viewer_service is None:
        _data_service = DataService(project_root=str(project_root))
        _viewer_service = NewsViewerService(
            project_root=str(project_root),
            data_service=_data_service
        )
    
    return _viewer_service, _data_service


async def fetch_news_data():
    """æ‰§è¡Œä¸€æ¬¡æ•°æ®è·å–"""
    global _last_fetch_time

    def _run_blocking_fetch():
        try:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ”„ å¼€å§‹è·å–æœ€æ–°æ•°æ®...")

            # åŠ è½½é…ç½®
            config = load_config(str(project_root / "config" / "config.yaml"))

            # è·å–å¹³å°åˆ—è¡¨ï¼ˆload_config è¿”å›çš„ key æ˜¯å¤§å†™ PLATFORMSï¼‰
            platforms_config = config.get("PLATFORMS", [])

            # å¤„ç†åˆ—è¡¨æ ¼å¼ï¼š[{id: "xxx", name: "xxx"}, ...]
            if isinstance(platforms_config, list):
                platforms = {p["id"]: p["name"] for p in platforms_config if isinstance(p, dict) and "id" in p}
            else:
                # å­—å…¸æ ¼å¼ï¼š{id: name, ...}
                platforms = platforms_config

            platform_ids = list(platforms.keys())

            if not platform_ids:
                print("âš ï¸ æœªé…ç½®ä»»ä½•å¹³å°")
                return {"success": False, "error": "æœªé…ç½®å¹³å°"}

            # åˆ›å»ºæ•°æ®è·å–å™¨
            crawler_config = config.get("CRAWLER", {})
            proxy_url = crawler_config.get("proxy_url") if crawler_config.get("use_proxy") else None
            api_url = crawler_config.get("api_url")
            fetcher = DataFetcher(proxy_url=proxy_url, api_url=api_url)

            # æ„å»ºå¹³å°IDå’Œåç§°çš„å…ƒç»„åˆ—è¡¨
            platform_tuples = [(pid, platforms[pid]) for pid in platform_ids]

            # æ‰¹é‡è·å–æ•°æ®ï¼ˆé˜»å¡è°ƒç”¨ï¼Œæ”¾åˆ°çº¿ç¨‹é‡Œæ‰§è¡Œï¼Œé¿å…å¡ä½äº‹ä»¶å¾ªç¯ï¼‰
            crawl_results, id_to_name, failed_ids = fetcher.crawl_websites(platform_tuples)

            now_str = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            batch_metrics = []
            for m in getattr(fetcher, "last_crawl_metrics", []) or []:
                if not isinstance(m, dict):
                    continue
                mm = dict(m)
                mm["fetched_at"] = now_str

                pid = str(mm.get("platform_id") or "").strip()
                content_keys = mm.pop("_content_keys", None)
                changed_count = None
                if pid and isinstance(content_keys, list):
                    prev = _last_platform_content_keys.get(pid)
                    if isinstance(prev, list) and prev:
                        prev_set = set(prev)
                        changed_count = sum(1 for k in content_keys if k not in prev_set)
                    else:
                        changed_count = len(content_keys)
                    _last_platform_content_keys[pid] = content_keys

                mm["changed_count"] = changed_count
                batch_metrics.append(mm)
            _record_fetch_metrics(batch_metrics)
            _append_fetch_metrics_batch(batch_metrics)

            if not crawl_results:
                print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ•°æ®")
                return {"success": False, "error": "æœªè·å–åˆ°æ•°æ®"}

            # è·å–å½“å‰æ—¶é—´
            now = datetime.now()
            crawl_time = now.strftime("%H:%M")
            crawl_date = now.strftime("%Y-%m-%d")

            # è½¬æ¢å¹¶ä¿å­˜æ•°æ®
            news_data = convert_crawl_results_to_news_data(
                crawl_results,
                id_to_name,
                failed_ids,
                crawl_time,
                crawl_date,
            )

            # è·å–å­˜å‚¨ç®¡ç†å™¨å¹¶ä¿å­˜
            from trendradar.storage import StorageManager

            # ä½¿ç”¨æ­£ç¡®çš„å­˜å‚¨é…ç½®åˆå§‹åŒ–
            storage_config = config.get("STORAGE", {})
            storage = StorageManager(
                backend_type=storage_config.get("backend", "local"),
                data_dir=str(project_root / storage_config.get("local", {}).get("data_dir", "output")),
                enable_txt=storage_config.get("formats", {}).get("txt", False),
                enable_html=storage_config.get("formats", {}).get("html", False),
            )
            storage.save_news_data(news_data)

            try:
                from trendradar.providers.runner import build_default_registry, run_provider_ingestion_once

                print(f"[{now.strftime('%H:%M:%S')}] ğŸ”„ è¿è¡Œ Provider Ingestion...")
                ok, metrics = run_provider_ingestion_once(
                    registry=build_default_registry(),
                    project_root=project_root,
                    config_path=project_root / "config" / "config.yaml",
                    now=now,
                )
                if metrics:
                    for m in metrics:
                        pid = m.get("platform_id", "?")
                        status = m.get("status", "?")
                        count = m.get("items_count", 0)
                        print(f"  - {pid}: {status} ({count} items)")
                else:
                    print("  - Provider Ingestion: æ— é…ç½®æˆ–å·²ç¦ç”¨")
            except Exception as e:
                print(f"[{now.strftime('%H:%M:%S')}] âš ï¸ Provider Ingestion å¤±è´¥: {e}")

            global _viewer_service, _data_service, _last_fetch_time
            _last_fetch_time = datetime.now()

            # æ¸…é™¤ç¼“å­˜ä»¥åŠ è½½æ–°æ•°æ®
            from mcp_server.services.cache_service import get_cache
            from trendradar.web.news_viewer import clear_categorized_news_cache

            cache = get_cache()
            cache.clear()  # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            clear_categorized_news_cache()  # æ¸…é™¤åˆ†ç±»æ–°é—»ç¼“å­˜

            # é‡ç½®æœåŠ¡å®ä¾‹
            _viewer_service = None
            _data_service = None

            total_news = sum(len(items) for items in crawl_results.values())
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ… æ•°æ®è·å–å®Œæˆ: {len(crawl_results)} ä¸ªå¹³å°, {total_news} æ¡æ–°é—»")

            return {"success": True, "platforms": len(crawl_results), "news_count": total_news}

        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âŒ æ•°æ®è·å–å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    return await asyncio.to_thread(_run_blocking_fetch)


@app.get("/api/fetch-metrics")
async def api_fetch_metrics(
    limit: int = Query(200, ge=1, le=_FETCH_METRICS_MAX),
    platform: Optional[str] = Query(None),
    provider: Optional[str] = Query(None),
):
    with _fetch_metrics_lock:
        items = list(_fetch_metrics)

    if provider:
        items = [m for m in items if str(m.get("provider") or "").strip() == provider]
    if platform:
        items = [m for m in items if str(m.get("platform_id") or "").strip() == platform]

    items = items[-limit:]

    summary = {}
    for m in items:
        pid = str(m.get("platform_id") or "").strip() or "unknown"
        ent = summary.get(pid)
        if ent is None:
            ent = {
                "platform_id": pid,
                "platform_name": m.get("platform_name") or pid,
                "provider": m.get("provider") or "",
                "success": 0,
                "cache": 0,
                "error": 0,
                "avg_duration_ms": None,
                "avg_items_count": None,
                "avg_changed_count": None,
                "last_status": None,
                "last_fetched_at": None,
                "last_changed_count": None,
                "last_content_hash": None,
            }
            summary[pid] = ent

        st = str(m.get("status") or "").strip()
        if st in ("success", "cache", "error"):
            ent[st] += 1
        else:
            ent["error"] += 1

        ent["last_status"] = st
        ent["last_fetched_at"] = m.get("fetched_at")
        ent["last_changed_count"] = m.get("changed_count")
        ent["last_content_hash"] = m.get("content_hash")

        dur = m.get("duration_ms")
        cnt = m.get("items_count")
        chg = m.get("changed_count")
        if isinstance(dur, (int, float)):
            ent.setdefault("_dur_sum", 0)
            ent.setdefault("_dur_n", 0)
            ent["_dur_sum"] += float(dur)
            ent["_dur_n"] += 1
        if isinstance(cnt, (int, float)):
            ent.setdefault("_cnt_sum", 0)
            ent.setdefault("_cnt_n", 0)
            ent["_cnt_sum"] += float(cnt)
            ent["_cnt_n"] += 1
        if isinstance(chg, (int, float)):
            ent.setdefault("_chg_sum", 0)
            ent.setdefault("_chg_n", 0)
            ent["_chg_sum"] += float(chg)
            ent["_chg_n"] += 1

    for ent in summary.values():
        dn = ent.pop("_dur_n", 0)
        ds = ent.pop("_dur_sum", 0)
        cn = ent.pop("_cnt_n", 0)
        cs = ent.pop("_cnt_sum", 0)
        hn = ent.pop("_chg_n", 0)
        hs = ent.pop("_chg_sum", 0)
        if dn:
            ent["avg_duration_ms"] = int(ds / dn)
        if cn:
            ent["avg_items_count"] = round(cs / cn, 2)
        if hn:
            ent["avg_changed_count"] = round(hs / hn, 2)

    return UnicodeJSONResponse(content={"limit": limit, "metrics": items, "summary": list(summary.values())})


async def scheduler_loop():
    """å®šæ—¶ä»»åŠ¡å¾ªç¯"""
    global _scheduler_running
    
    while _scheduler_running:
        await fetch_news_data()
        
        # ç­‰å¾…ä¸‹ä¸€æ¬¡æ‰§è¡Œ
        print(f"â° ä¸‹æ¬¡è·å–æ—¶é—´: {_fetch_interval_minutes} åˆ†é’Ÿå")
        await asyncio.sleep(_fetch_interval_minutes * 60)


def start_scheduler(interval_minutes: int = 30):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task, _scheduler_running, _fetch_interval_minutes
    
    if _scheduler_running:
        print("âš ï¸ å®šæ—¶ä»»åŠ¡å·²åœ¨è¿è¡Œä¸­")
        return
    
    _fetch_interval_minutes = interval_minutes
    _scheduler_running = True
    _scheduler_task = asyncio.create_task(scheduler_loop())
    print(f"âœ… å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš”: {interval_minutes} åˆ†é’Ÿ")


def stop_scheduler():
    """åœæ­¢å®šæ—¶ä»»åŠ¡"""
    global _scheduler_task, _scheduler_running
    
    _scheduler_running = False
    if _scheduler_task:
        _scheduler_task.cancel()
        _scheduler_task = None
    print("â¹ï¸ å®šæ—¶ä»»åŠ¡å·²åœæ­¢")


def _get_cdn_base_url() -> str:
    """è·å– CDN åŸºç¡€ URL"""
    try:
        import yaml
        config_path = project_root / "config" / "config.yaml"
        with open(config_path, "r", encoding="utf-8") as f:
            full_config = yaml.safe_load(f) or {}
        viewer_config = full_config.get("viewer", {}) or {}
        return (viewer_config.get("cdn_base_url") or "").strip()
    except Exception:
        return ""


def _read_user_config_from_cookie(request: Request) -> Optional[dict]:
    """ä» Cookie è¯»å–ç”¨æˆ·é…ç½®"""
    try:
        cookie_value = request.cookies.get("trendradar_config")
        if not cookie_value:
            return None
        
        # è§£ç å¹¶è§£æ JSON
        decoded = unquote(cookie_value)
        config = json.loads(decoded)
        
        # éªŒè¯ç‰ˆæœ¬
        if config.get("v") != 1:
            return None
        
        return config
    except Exception as e:
        print(f"Failed to read user config from cookie: {e}")
        return None


def _apply_user_config_to_data(data: dict, user_config: dict) -> dict:
    """åº”ç”¨ç”¨æˆ·é…ç½®åˆ°æ•°æ®"""
    try:
        categories = data.get("categories", {})
        if not categories:
            return data
        
        # è·å–é…ç½®
        custom_categories = user_config.get("custom", [])
        hidden_categories = user_config.get("hidden", [])
        category_order = user_config.get("order", [])
        
        # æ„å»ºæ–°çš„åˆ†ç±»å­—å…¸
        result_categories = {}
        
        # æŒ‰ç…§ç”¨æˆ·å®šä¹‰çš„é¡ºåºå¤„ç†
        for cat_id in category_order:
            # è·³è¿‡éšè—çš„åˆ†ç±»
            if cat_id in hidden_categories:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è‡ªå®šä¹‰åˆ†ç±»
            custom_cat = next((c for c in custom_categories if c.get("id") == cat_id), None)
            
            if custom_cat:
                # è‡ªå®šä¹‰åˆ†ç±»ï¼šä»æ‰€æœ‰å¹³å°ä¸­ç­›é€‰
                platforms = {}
                for platform_id in custom_cat.get("platforms", []):
                    # åœ¨æ‰€æœ‰é»˜è®¤åˆ†ç±»ä¸­æŸ¥æ‰¾è¯¥å¹³å°
                    for cat in categories.values():
                        if platform_id in cat.get("platforms", {}):
                            platforms[platform_id] = cat["platforms"][platform_id]
                            break
                
                if platforms:
                    result_categories[cat_id] = {
                        "name": custom_cat.get("name", cat_id),
                        "icon": "ğŸ“±",
                        "platforms": platforms
                    }
            elif cat_id in categories:
                # é»˜è®¤åˆ†ç±»ï¼šç›´æ¥ä½¿ç”¨
                result_categories[cat_id] = categories[cat_id]
        
        # æ·»åŠ æœªåœ¨ order ä¸­çš„é»˜è®¤åˆ†ç±»ï¼ˆä½†ä¸åœ¨ hidden ä¸­ï¼‰
        for cat_id, cat_data in categories.items():
            if cat_id not in result_categories and cat_id not in hidden_categories:
                result_categories[cat_id] = cat_data
        
        # æ›´æ–°æ•°æ®
        data["categories"] = result_categories
        return data
        
    except Exception as e:
        print(f"Failed to apply user config: {e}")
        return data


async def _render_viewer_page(
    request: Request,
    filter: Optional[str],
    platforms: Optional[str],
):
    viewer_service, _ = get_services()

    platform_list = None
    if platforms:
        platform_list = [p.strip() for p in platforms.split(",") if p.strip()]

    try:
        data = viewer_service.get_categorized_news(
            platforms=platform_list,
            limit=5000,
            apply_filter=True,
            filter_mode=filter,
        )

        # è·å– CDN é…ç½®
        cdn_base_url = _get_cdn_base_url()
        static_prefix = cdn_base_url if cdn_base_url else "/static"

        return templates.TemplateResponse(
            "viewer.html",
            {
                "request": request,
                "data": data,
                "available_filters": ["strict", "moderate", "off"],
                "current_filter": filter or data.get("filter_mode", "moderate"),
                "static_prefix": static_prefix,
            },
        )
    except Exception as e:
        return HTMLResponse(
            content=f"""
            <html>
                <head><title>é”™è¯¯</title></head>
                <body>
                    <h1>åŠ è½½å¤±è´¥</h1>
                    <p>é”™è¯¯ä¿¡æ¯: {str(e)}</p>
                    <p>è¯·ç¡®ä¿å·²ç»è¿è¡Œè¿‡çˆ¬è™«å¹¶æœ‰æ–°é—»æ•°æ®ã€‚</p>
                </body>
            </html>
            """,
            status_code=500,
        )


@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    return await _render_viewer_page(request, filter=None, platforms=None)


@app.get("/viewer", response_class=HTMLResponse)
async def viewer(
    request: Request,
    filter: Optional[str] = Query(None, description="è¿‡æ»¤æ¨¡å¼: strict/moderate/off"),
    platforms: Optional[str] = Query(None, description="å¹³å°åˆ—è¡¨ï¼Œé€—å·åˆ†éš”")
):
    """
    æ–°é—»åˆ†ç±»æŸ¥çœ‹å™¨ä¸»é¡µé¢
    
    Args:
        filter: ä¸´æ—¶è¦†ç›–è¿‡æ»¤æ¨¡å¼
        platforms: æŒ‡å®šè¦æŸ¥çœ‹çš„å¹³å°ï¼ˆé€—å·åˆ†éš”ï¼‰
    """
    return await _render_viewer_page(request, filter=filter, platforms=platforms)


@app.get("/api/news")
async def api_news(
    platforms: Optional[str] = Query(None),
    limit: int = Query(5000, ge=1, le=10000),
    filter_mode: Optional[str] = Query(None)
):
    """API: è·å–åˆ†ç±»æ–°é—»æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰"""
    viewer_service, _ = get_services()
    
    platform_list = None
    if platforms:
        platform_list = [p.strip() for p in platforms.split(",") if p.strip()]
    
    data = viewer_service.get_categorized_news(
        platforms=platform_list,
        limit=limit,
        apply_filter=True,
        filter_mode=filter_mode
    )

    return UnicodeJSONResponse(content=data)


@app.get("/api/nba-today")
async def api_nba_today():
    today = date.today().strftime("%Y-%m-%d")
    url = f"https://matchweb.sports.qq.com/kbs/list?columnId=100000&startTime={today}&endTime={today}"

    def _fetch():
        resp = requests.get(
            url,
            headers={
                "Referer": "https://kbs.sports.qq.com/",
                "User-Agent": "Mozilla/5.0",
                "Accept": "application/json, text/plain, */*",
            },
            timeout=10,
        )
        resp.raise_for_status()
        return resp.json()

    try:
        payload = await asyncio.to_thread(_fetch)
    except Exception as e:
        return JSONResponse(content={"detail": f"Failed to fetch Tencent NBA data: {e}"}, status_code=502)

    from trendradar.providers.tencent_nba import _extract_tencent_nba_matches

    games = _extract_tencent_nba_matches(payload)
    return UnicodeJSONResponse(content={"date": today, "games": games})


@app.post("/api/online/ping")
async def online_ping(request: Request):
    body = {}
    try:
        body = await request.json()
    except Exception:
        body = {}

    session_id = (body.get("session_id") or "").strip()
    if not session_id:
        return JSONResponse(content={"detail": "Missing session_id"}, status_code=400)

    now = int(time.time())
    conn = _get_online_db_conn()
    conn.execute(
        "INSERT OR REPLACE INTO online_sessions(session_id, last_seen) VALUES (?, ?)",
        (session_id, now),
    )
    conn.execute("DELETE FROM online_sessions WHERE last_seen < ?", (now - 86400,))
    conn.commit()

    return JSONResponse(content={"ok": True})


@app.get("/api/online")
async def online_stats():
    now = int(time.time())
    conn = _get_online_db_conn()

    def count_since(seconds: int) -> int:
        cur = conn.execute(
            "SELECT COUNT(*) FROM online_sessions WHERE last_seen >= ?",
            (now - seconds,),
        )
        row = cur.fetchone()
        return int(row[0] if row else 0)

    stats = {
        "online_1m": count_since(60),
        "online_5m": count_since(5 * 60),
        "online_15m": count_since(15 * 60),
        "server_time": now,
    }

    return JSONResponse(content=stats)


@app.get("/api/categories")
async def api_categories():
    """API: è·å–åˆ†ç±»åˆ—è¡¨"""
    viewer_service, _ = get_services()
    categories = viewer_service.get_category_list()
    return UnicodeJSONResponse(content=categories)


@app.get("/api/filter/stats")
async def api_filter_stats():
    """API: è·å–è¿‡æ»¤ç»Ÿè®¡"""
    viewer_service, _ = get_services()
    stats = viewer_service.get_filter_stats()
    return UnicodeJSONResponse(content=stats)


@app.post("/api/filter/mode")
async def api_set_filter_mode(mode: str):
    """API: è®¾ç½®è¿‡æ»¤æ¨¡å¼"""
    viewer_service, _ = get_services()
    success = viewer_service.set_filter_mode(mode)
    return UnicodeJSONResponse(content={"success": success, "mode": mode})


@app.get("/api/blacklist/keywords")
async def api_blacklist_keywords():
    """API: è·å–é»‘åå•å…³é”®è¯"""
    viewer_service, _ = get_services()
    keywords = viewer_service.get_blacklist_keywords()
    return UnicodeJSONResponse(content={"keywords": keywords})


@app.post("/api/blacklist/reload")
async def api_reload_blacklist():
    """API: é‡æ–°åŠ è½½é»‘åå•"""
    viewer_service, _ = get_services()
    count = viewer_service.reload_blacklist()
    return UnicodeJSONResponse(content={"success": True, "keywords_count": count})


@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "TrendRadar News Viewer",
        "health_schema": "2",
        "version": os.environ.get("APP_VERSION", "unknown"),
        "config_rev": os.environ.get("CONFIG_REV", "0"),
    }


# === å®šæ—¶ä»»åŠ¡ç›¸å…³ API ===

@app.post("/api/scheduler/start")
async def api_start_scheduler(interval: int = Query(30, ge=5, le=1440)):
    """
    å¯åŠ¨å®šæ—¶æ•°æ®è·å–ä»»åŠ¡
    
    Args:
        interval: è·å–é—´éš”ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤30ï¼ŒèŒƒå›´5-1440
    """
    start_scheduler(interval)
    return UnicodeJSONResponse(content={
        "success": True,
        "message": f"å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œé—´éš” {interval} åˆ†é’Ÿ",
        "interval_minutes": interval
    })


@app.post("/api/scheduler/stop")
async def api_stop_scheduler():
    """åœæ­¢å®šæ—¶æ•°æ®è·å–ä»»åŠ¡"""
    stop_scheduler()
    return UnicodeJSONResponse(content={
        "success": True,
        "message": "å®šæ—¶ä»»åŠ¡å·²åœæ­¢"
    })


@app.get("/api/scheduler/status")
async def api_scheduler_status():
    """è·å–å®šæ—¶ä»»åŠ¡çŠ¶æ€"""
    return UnicodeJSONResponse(content={
        "running": _scheduler_running,
        "interval_minutes": _fetch_interval_minutes,
        "last_fetch_time": _last_fetch_time.isoformat() if _last_fetch_time else None
    })


@app.post("/api/fetch")
async def api_fetch_now():
    """ç«‹å³æ‰§è¡Œä¸€æ¬¡æ•°æ®è·å–"""
    result = await fetch_news_data()
    try:
        get_cache().clear()
    except Exception:
        pass
    return UnicodeJSONResponse(content=result)


async def _warmup_cache():
    """é¢„çƒ­ç¼“å­˜ï¼šåœ¨æœåŠ¡å¯åŠ¨æ—¶é¢„åŠ è½½æ•°æ®"""
    try:
        print("ğŸ”¥ é¢„çƒ­ç¼“å­˜ä¸­...")
        start_time = time.time()
        
        # é¢„åŠ è½½æ–°é—»æ•°æ®åˆ°ç¼“å­˜
        viewer_service, _ = get_services()
        viewer_service.get_categorized_news(
            platforms=None,
            limit=5000,
            apply_filter=True,
            filter_mode=None
        )
        
        elapsed = time.time() - start_time
        print(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ ({elapsed:.2f}s)")
    except Exception as e:
        print(f"âš ï¸ ç¼“å­˜é¢„çƒ­å¤±è´¥: {e}")


@app.on_event("startup")
async def on_startup():
    """æœåŠ¡å™¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    # 1. é¢„çƒ­ç¼“å­˜
    await _warmup_cache()
    
    # 2. è¯»å–é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨å¯åŠ¨å®šæ—¶ä»»åŠ¡
    try:
        import yaml
        config_path = project_root / "config" / "config.yaml"
        with open(config_path, "r", encoding="utf-8") as f:
            full_config = yaml.safe_load(f) or {}
        viewer_config = full_config.get("viewer", {}) or {}
        
        auto_fetch = viewer_config.get("auto_fetch", False)
        fetch_interval = viewer_config.get("fetch_interval_minutes", 30)
        
        if auto_fetch:
            print(f"ğŸ“… è‡ªåŠ¨å¯åŠ¨å®šæ—¶è·å–ä»»åŠ¡ (é—´éš”: {fetch_interval} åˆ†é’Ÿ)")
            start_scheduler(fetch_interval)

            # scheduler_loop æœ¬èº«ä¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ fetch_news_data()ï¼Œé¿å…å¯åŠ¨æ—¶é‡å¤è§¦å‘
    except Exception as e:
        print(f"âš ï¸ è¯»å–é…ç½®å¤±è´¥ï¼Œè·³è¿‡è‡ªåŠ¨å®šæ—¶ä»»åŠ¡: {e}")


def run_server(host: str = "0.0.0.0", port: int = 8080, auto_fetch: bool = False, interval: int = 30):
    """è¿è¡Œ Web æœåŠ¡å™¨"""
    import uvicorn
    
    print("=" * 60)
    print("ğŸš€ TrendRadar News Viewer Server")
    print("=" * 60)
    print(f"ğŸ“¡ Server Address: http://{host}:{port}")
    print(f"ğŸŒ Viewer URL: http://localhost:{port}/viewer")
    print(f"ğŸ“Š API Docs: http://localhost:{port}/docs")
    print("-" * 60)
    print("ğŸ“Œ å®šæ—¶ä»»åŠ¡ API:")
    print(f"   POST /api/scheduler/start?interval=30  å¯åŠ¨å®šæ—¶è·å–")
    print(f"   POST /api/scheduler/stop               åœæ­¢å®šæ—¶è·å–")
    print(f"   GET  /api/scheduler/status             æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€")
    print(f"   POST /api/fetch                        ç«‹å³è·å–ä¸€æ¬¡")
    print("=" * 60)
    print()
    
    uvicorn.run(app, host=host, port=port)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="TrendRadar News Viewer Server")
    parser.add_argument("--host", default="0.0.0.0", help="ç›‘å¬åœ°å€")
    parser.add_argument("--port", type=int, default=8080, help="ç›‘å¬ç«¯å£")
    
    args = parser.parse_args()
    run_server(host=args.host, port=args.port)
